{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db739f1f",
   "metadata": {},
   "source": [
    "# Connection\n",
    "This notebook acts as the connection between my local development environment and my Google Colab virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9293641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hello.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile hello.cu\n",
    "#include <iostream>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// This is the 'Kernel' - it runs on the GPU\n",
    "__global__ void hello_from_gpu() {\n",
    "    int block_id = blockIdx.x;\n",
    "    int thread_id = threadIdx.x;\n",
    "    printf(\"Hello from GPU! Block: %d, Thread: %d\\n\", block_id, thread_id);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    std::cout << \"Hello from the CPU!\" << std::endl;\n",
    "\n",
    "    // Launch the kernel: 2 blocks, 4 threads each\n",
    "    // Syntax: kernel_name<<<blocks, threads_per_block>>>();\n",
    "    hello_from_gpu<<<2, 4>>>();\n",
    "\n",
    "    // Wait for the GPU to finish before the CPU continues\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c580b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from the CPU!\n",
      "Hello from GPU! Block: 0, Thread: 0\n",
      "Hello from GPU! Block: 0, Thread: 1\n",
      "Hello from GPU! Block: 0, Thread: 2\n",
      "Hello from GPU! Block: 0, Thread: 3\n",
      "Hello from GPU! Block: 1, Thread: 0\n",
      "Hello from GPU! Block: 1, Thread: 1\n",
      "Hello from GPU! Block: 1, Thread: 2\n",
      "Hello from GPU! Block: 1, Thread: 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!nvcc hello.cu -o hello -arch=sm_75\n",
    "!./hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca5726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hello_cuda.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile hello_cuda.cu\n",
    "#include <iostream>\n",
    "#include <cstdlib>\n",
    "\n",
    "\n",
    "__global__ void my_first_kernel(int* A, int* B, int* C) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // The \"Guard\": Only do work if we are inside the array bounds\n",
    "    if (i < n) {\n",
    "        C[i] = A[i] + B[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // allocate memory on the host and fill with data\n",
    "    size_t s = 100000000;\n",
    "    int* a = (int*) malloc(sizeof(int)*s);\n",
    "    int* b = (int*) malloc(sizeof(int)*s);\n",
    "    int* c = (int*) malloc(sizeof(int)*s);\n",
    "\n",
    "    for (int i = 0; i < s; i++){\n",
    "        a[i] = i; b[i] = i;\n",
    "    }\n",
    "\n",
    "    // allocate memory on the device \n",
    "    int *d_a, *d_b, *d_c;\n",
    "\n",
    "    cudaMalloc((void**)&d_a, sizeof(int)*s);\n",
    "    cudaMalloc((void**)&d_b, sizeof(int)*s);\n",
    "    cudaMalloc((void**)&d_c, sizeof(int)*s);\n",
    "\n",
    "    // copy the data from host to device\n",
    "    // cudaMemcpy(destination, source, size, direction);\n",
    "    cudaMemcpy(d_a, a, sizeof(int)*s, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, b, sizeof(int)*s, cudaMemcpyHostToDevice);\n",
    "\n",
    "    // launch\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = (s + threadsPerBlock - 1) / threadsPerBlock;\n",
    "\n",
    "    my_first_kernel<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c);\n",
    "\n",
    "    cudaMemcpy(c, d_c, sizeof(int)*s, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // Wait for GPU to finish\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    for (int i = 0; i < 3; ++i){\n",
    "        std::cout << c[i] << std::endl;\n",
    "    }\n",
    "\n",
    "    // Free all memory allocations\n",
    "    free(a);\n",
    "    free(b);\n",
    "    free(c);\n",
    "    cudaFree(d_a);\n",
    "    cudaFree(d_b);\n",
    "    cudaFree(d_c);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53fd9d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "!nvcc hello_cuda.cu -o hello_cuda -arch=sm_75\n",
    "!./hello_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa41ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
